{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a056ec",
   "metadata": {},
   "source": [
    "## Working with Images\n",
    "\n",
    "In this tutorial, we'll use our existing knowledge of PyTorch and linear regression to solve a very different kind of problem: *image classification*. We'll use the famous [*MNIST Handwritten Digits Database*](http://yann.lecun.com/exdb/mnist/) as our training dataset. It consists of 28px by 28px grayscale images of handwritten digits (0 to 9) and labels for each image indicating which digit it represents. Here are some sample images from the dataset:\n",
    "\n",
    "![mnist-sample](https://i.imgur.com/CAYnuo1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f2d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7d7ef",
   "metadata": {},
   "source": [
    "üß† What is torchvision in PyTorch?\n",
    "\n",
    "torchvision is an official PyTorch library that provides tools, datasets, and pretrained models specifically designed for computer vision tasks like:\n",
    "\n",
    "Image classification\n",
    "\n",
    "Object detection\n",
    "\n",
    "Image segmentation\n",
    "\n",
    "Image transformations and preprocessing\n",
    "\n",
    "It helps you work with image data easily ‚Äî instead of writing all the low-level code yourself.\n",
    "\n",
    "‚öôÔ∏è Main Components of torchvision\n",
    "| Component                    | Description                                                   | Example                                                               |\n",
    "| ---------------------------- | ------------------------------------------------------------- | --------------------------------------------------------------------- |\n",
    "| **`torchvision.datasets`**   | Ready-to-use popular datasets for training/testing            | `CIFAR10`, `MNIST`, `ImageNet`, etc.                                  |\n",
    "| **`torchvision.transforms`** | Image preprocessing utilities (resize, crop, normalize, etc.) | `transforms.Compose([transforms.Resize(256), transforms.ToTensor()])` |\n",
    "| **`torchvision.models`**     | Pretrained deep learning models for vision tasks              | `resnet50`, `vgg16`, `mobilenet_v3`, etc.                             |\n",
    "| **`torchvision.io`**         | Functions for reading and writing images/videos               | `torchvision.io.read_image(\"img.png\")`                                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c91ab35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:01<00:00, 6.07MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 161kB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:01<00:00, 1.52MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 10.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST(root='/content/data', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd4dd6a",
   "metadata": {},
   "source": [
    "not only downloaded the file it automatically converts to pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad1189c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8972cb4a",
   "metadata": {},
   "source": [
    "The dataset has 60,000 images that we'll use to train the model. There is also an additional test set of 10,000 images used for evaluating models and reporting metrics in papers and reports. We can create the test dataset using the `MNIST` class by passing `train=False` to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "841e4ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root = '/content/data',train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d69adcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664b7bd",
   "metadata": {},
   "source": [
    "## üß† How MNIST Dataset Works in PyTorch\n",
    "\n",
    "When we download MNIST using:\n",
    "```python\n",
    "from torchvision.datasets import MNIST\n",
    "dataset = MNIST(root='/content/data', download=True)\n",
    "PyTorch automatically:\n",
    "\n",
    "Downloads raw binary files (.ubyte format).\n",
    "\n",
    "Converts them to tensors and saves as:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "/data/MNIST/processed/\n",
    "‚îú‚îÄ‚îÄ training.pt\n",
    "‚îî‚îÄ‚îÄ test.pt\n",
    "Each .pt file contains a tuple of tensors:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "(images_tensor, labels_tensor)\n",
    "üñºÔ∏è Image Tensor Details\n",
    "Shape: [60000, 28, 28] for training data\n",
    "\n",
    "Each pixel ‚Üí brightness value\n",
    "\n",
    "0 ‚Üí black\n",
    "\n",
    "255 ‚Üí white\n",
    "\n",
    "In-between ‚Üí gray levels\n",
    "\n",
    "After transforms.ToTensor(), values are normalized to range [0.0, 1.0]\n",
    "\n",
    "üè∑Ô∏è Label Tensor Details\n",
    "Shape: [60000]\n",
    "\n",
    "Contains digit class labels (0‚Äì9)\n",
    "\n",
    "üßÆ Example in Code\n",
    "python\n",
    "Copy code\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "dataset = MNIST(root='/content/data', download=True, transform=transforms.ToTensor())\n",
    "\n",
    "img, label = dataset[0]\n",
    "print(type(img), img.shape)   # torch.Tensor, torch.Size([1, 28, 28])\n",
    "print(label)                  # Example: 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55d30a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAG+FJREFUeJzt3X901PW95/HXBMiAmkwMIb9KwAAKPfLDK4WYRSlKDiH2cASxR9RuoevVlQaOQK02PQpiuzeKu61ri7rt7YF6j4ByV+DItuxiMOGoCS5BlmXbZkmalrAkodBmJgQTYvLZP1JHRwL6HSZ5Z8Lzcc73HDLzfWc+fh15+s3MfONzzjkBANDPEqwXAAC4MhEgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgYqj1Aj6vu7tbJ0+eVFJSknw+n/VyAAAeOefU2tqq7OxsJSRc/DxnwAXo5MmTysnJsV4GAOAyNTQ0aPTo0Re9f8AFKCkp6e9/8v19AwDEFyfJfebv89712WtAGzdu1HXXXafhw4crLy9PH3zwwZea+/THbj42NjY2trjd9IUvo/RJgF5//XWtWbNG69at06FDhzRt2jQVFhbq1KlTffFwAIA45OuLq2Hn5eVpxowZ+vnPfy6p540FOTk5WrlypX7wgx9ccjYUCikQCKinjZeuJwBgIHKSuhUMBpWcnHzRvWJ+BnT+/HlVV1eroKDg0wdJSFBBQYEqKysv2L+jo0OhUChiAwAMfjEP0OnTp9XV1aWMjIyI2zMyMtTU1HTB/qWlpQoEAuGNd8ABwJXB/IOoJSUlCgaD4a2hocF6SQCAfhDzt2GnpaVpyJAham5ujri9ublZmZmZF+zv9/vl9/tjvQwAwAAX8zOgxMRETZ8+XWVlZeHburu7VVZWpvz8/Fg/HAAgTvXJB1HXrFmjpUuX6mtf+5pmzpypF154QW1tbfrOd77TFw8HAIhDfRKge++9V3/5y1+0du1aNTU16aabbtKePXsueGMCAODK1SefA7ocfA4IAOKd0eeAAAD4MggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATQ60XAAwkvij+k7h6xNg+WElsbJ18T1RzKYnnPc/8w5RGzzO5W+o8z/zhGxM9zyT/yzLPM5LkCwU9z/z3+Qc8z3zjgw2eZwYDzoAAACYIEADARMwD9PTTT8vn80VskyZNivXDAADiXJ+8BnTjjTfq7bff/vRBhvJSEwAgUp+UYejQocrMzOyLbw0AGCT65DWgY8eOKTs7W+PGjdMDDzyg48ePX3Tfjo4OhUKhiA0AMPjFPEB5eXnavHmz9uzZo5dffln19fW67bbb1Nra2uv+paWlCgQC4S0nJyfWSwIADEAxD1BRUZG++c1vaurUqSosLNRvfvMbtbS06I033uh1/5KSEgWDwfDW0NAQ6yUBAAagPn93QEpKim644QbV1tb2er/f75ff7+/rZQAABpg+/xzQ2bNnVVdXp6ysrL5+KABAHIl5gB577DFVVFToT3/6k95//30tWrRIQ4YM0X333RfrhwIAxLGY/wjuxIkTuu+++3TmzBmNGjVKt956q6qqqjRq1KhYPxQAII7FPEDbtm2L9bfEAJWdMtvzzHBfwPPMvx35D55nlt5w0vOMJKWmn/M8c/V/+XZUjzXYDKk+5HmmrtPneWbES9/yPONr9n6hVEnS62WeR35Zy2cgvyyuBQcAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmOjzX0iHgS8vsDyquferp3ue6R49OqrHQv/yffyx55mnv93ueeavHRM8z2jCe55Hfh9q8/44khoSznqe+b9/ezaqx7oScQYEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE1wNG/o/5/9HVHO+P6Z5H+Jq2JIk3y+2e575+Pg5zzNDS+7xPCNJavN+9ehnap+J7rFwxeIMCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwcVIobMf1UU1V/wd7zM/uMn7RTjfqs/2PPPvD83yPBOthG1veZ4ZvnqX55nOj097npnyy+j+E9/1b/xRzQFecAYEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjwOeec9SI+KxQKKRAIqKeNPuvlIMaGJ3q/sGj7+UbPM3+880HPM5KUs+MezzNrv/q+55nSPz7jeQaIH05St4LBoJKTky+6F2dAAAATBAgAYMJzgPbv368FCxYoOztbPp9PO3fujLjfOae1a9cqKytLI0aMUEFBgY4dOxar9QIABgnPAWpra9O0adO0cePGXu/fsGGDXnzxRb3yyis6cOCArr76ahUWFqq9vf2yFwsAGDw8/7rEoqIiFRUV9Xqfc04vvPCCnnzySd11112SpFdffVUZGRnauXOnlixZcnmrBQAMGjF9Dai+vl5NTU0qKCgI3xYIBJSXl6fKyspeZzo6OhQKhSI2AMDgF9MANTU1SZIyMjIibs/IyAjf93mlpaUKBALhLScnJ5ZLAgAMUObvgispKVEwGAxvDQ0N1ksCAPSDmAYoMzNTktTc3Bxxe3Nzc/i+z/P7/UpOTo7YAACDX0wDlJubq8zMTJWVlYVvC4VCOnDggPLz82P5UACAOOf5XXBnz55VbW1t+Ov6+nodPnxYqampGjNmjFatWqUf//jHuv7665Wbm6unnnpK2dnZWrhwYSzXDQCIc54DdPDgQd1+++3hr9esWSNJWrp0qTZv3qzHH39cbW1tevjhh9XS0qJbb71Ve/bs0fDhw2O3agBA3ONipBiUqm57NKq56W/3/hm3S3r+Nc8jw57c4v1x1BXFDGCBi5ECAAYwAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmPD86xiAeHBbpfcrVEvSuef/6n3o+w94Hrn/P4/xPLPlL//B8wwwkHEGBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY8DnnnPUiPisUCikQCKinjT7r5eAKc921d3qeqT2yxPNMQsMJzzNNz9d4ntlxzPtFTyWp+OiPo5gaUH+VwJST1K1gMKjk5OSL7sUZEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABggouRApepePRTnmdefG+855nuzEzPM9H611v2e55ZWbvH88zp1kOeZxAPuBgpAGAAI0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcDFSwMBNKf/O88yBle2eZxKe/LbnmWj9ccEOzzO3VR32PHMqdMDzDPobFyMFAAxgBAgAYMJzgPbv368FCxYoOztbPp9PO3fujLh/2bJl8vl8Edv8+fNjtV4AwCDhOUBtbW2aNm2aNm7ceNF95s+fr8bGxvC2devWy1okAGDwGep1oKioSEVFRZfcx+/3K7Mff3sjACD+9MlrQOXl5UpPT9fEiRO1fPlynTlz5qL7dnR0KBQKRWwAgMEv5gGaP3++Xn31VZWVlem5555TRUWFioqK1NXV1ev+paWlCgQC4S0nJyfWSwIADECefwT3RZYsWRL+85QpUzR16lSNHz9e5eXlmjt37gX7l5SUaM2aNeGvQ6EQEQKAK0Cfvw173LhxSktLU21tba/3+/1+JScnR2wAgMGvzwN04sQJnTlzRllZWX39UACAOOL5R3Bnz56NOJupr6/X4cOHlZqaqtTUVK1fv16LFy9WZmam6urq9Pjjj2vChAkqLCyM6cIBAPHNc4AOHjyo22+/Pfz1J6/fLF26VC+//LKOHDmiX//612ppaVF2drbmzZunH/3oR/L7/bFbNQAg7nExUiBOXOUf63nmsZylUT3WU//7Fu9DCd5/ou/+6V88zySuf83zDPobFyMFAAxgBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHVsAFc4OPgTs8zbvhwzzO+9nbPM/fkVnme2fnXZz3P4HJwNWwAwABGgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJgYar0A4Ep0c8o/ep4pnXit55mvzznheUaK7sKi0fC9VeZ5ZudfX+qDlcACZ0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkuRgp8xg3X3uN5Zuf0UZ5nJj43wfNM9+QbPc/0J9/5855nOv/X6SgeqSuKGQxEnAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GCkGvNSkmzzP/Kdx34jqsZb+dJjnma5Zt3ie6fY80b8SNr/peWb5+gzPM784udXzDAYPzoAAACYIEADAhKcAlZaWasaMGUpKSlJ6eroWLlyompqaiH3a29tVXFyskSNH6pprrtHixYvV3Nwc00UDAOKfpwBVVFSouLhYVVVV2rt3rzo7OzVv3jy1tbWF91m9erXeeustbd++XRUVFTp58qTuvvvumC8cABDfPL0JYc+ePRFfb968Wenp6aqurtbs2bMVDAb1q1/9Slu2bNEdd9whSdq0aZO++tWvqqqqSrfc4v3FWgDA4HRZrwEFg0FJUmpqqiSpurpanZ2dKigoCO8zadIkjRkzRpWVlb1+j46ODoVCoYgNADD4RR2g7u5urVq1SrNmzdLkyZMlSU1NTUpMTFRKSkrEvhkZGWpqaur1+5SWlioQCIS3nJycaJcEAIgjUQeouLhYR48e1bZt2y5rASUlJQoGg+GtoaHhsr4fACA+RPVB1BUrVmj37t3av3+/Ro8eHb49MzNT58+fV0tLS8RZUHNzszIzM3v9Xn6/X36/P5plAADimKczIOecVqxYoR07dmjfvn3Kzc2NuH/69OkaNmyYysrKwrfV1NTo+PHjys/Pj82KAQCDgqczoOLiYm3ZskW7du1SUlJS+HWdQCCgESNGKBAI6MEHH9SaNWuUmpqq5ORkrVy5Uvn5+bwDDgAQwVOAXn75ZUnSnDlzIm7ftGmTli1bJkn66U9/qoSEBC1evFgdHR0qLCzUSy+9FJPFAgAGD59zzlkv4rNCoZACgYB6fjros14OLuHaa6Z6nrnD7/0iodv/+WPPM113zvU8M9D5frHd88yjz2VH9VgbT/xTFFNdUT0WBiMnqVvBYFDJyckX3YtrwQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMBEVL8RFQPXNSPGe575f0tmRvVYSfdP8DzTNXuW9xnPE/3Lt9H7r6Vf+ZMczzO/bPyvnmc+7mrxPAP0F86AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATXIy0n9wRWO15Zuc9TZ5nrvnHaZ5num6+yfOMNLAvEur729+imtu/6KDnmcL/udvzTOfHpz3PAIMNZ0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkuRtpP1k/2PjPipaWeZ/rzAqFD/ttezzPv/kfneabTef//pAWHtnuekaRzHX+Oag6Ad5wBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmfM4571eH7EOhUEiBQEA9bfRZLwcA4JmT1K1gMKjk5OSL7sUZEADABAECAJjwFKDS0lLNmDFDSUlJSk9P18KFC1VTUxOxz5w5c+Tz+SK2Rx55JKaLBgDEP08BqqioUHFxsaqqqrR37151dnZq3rx5amtri9jvoYceUmNjY3jbsGFDTBcNAIh/nn4j6p49eyK+3rx5s9LT01VdXa3Zs2eHb7/qqquUmZkZmxUCAAaly3oNKBgMSpJSU1Mjbn/ttdeUlpamyZMnq6SkROfOnbvo9+jo6FAoFIrYAACDn6czoM/q7u7WqlWrNGvWLE2ePDl8+/3336+xY8cqOztbR44c0RNPPKGamhq9+eabvX6f0tJSrV+/PtplAADiVNSfA1q+fLl++9vf6t1339Xo0aMvut++ffs0d+5c1dbWavz48Rfc39HRoY6OjvDXoVBIOTk54nNAABCvvtzngKI6A1qxYoV2796t/fv3XzI+kpSXlydJFw2Q3++X3++PZhkAgDjmKUDOOa1cuVI7duxQeXm5cnNzv3Dm8OHDkqSsrKyoFggAGJw8Bai4uFhbtmzRrl27lJSUpKamJklSIBDQiBEjVFdXpy1btujOO+/UyJEjdeTIEa1evVqzZ8/W1KlT++QfAAAQnzy9BuTz9f6azKZNm7Rs2TI1NDToW9/6lo4ePaq2tjbl5ORo0aJFevLJJy/5c8DP4lpwABDvvtxrQFyMFAAQY1yMFAAwgBEgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATAy1XsDnOec++ZPpOgAA0er5+/vTv897N+AC1Nra+vc/OREhAIhfra2tCgQCF73f574oUf2su7tbJ0+eVFJSknw+X8R9oVBIOTk5amhoUHJystEK7XEcenAcenAcenAcegyE4+CcU2trq7Kzs5WQcPFXegbcGVBCQoJGjx59yX2Sk5Ov6CfYJzgOPTgOPTgOPTgOPayPw6XOfD7BmxAAACYIEADARFwFyO/3a926dfL7/dZLMcVx6MFx6MFx6MFx6BFPx2HAvQkBAHBliKszIADA4EGAAAAmCBAAwAQBAgCYiJsAbdy4Udddd52GDx+uvLw8ffDBB9ZL6ndPP/20fD5fxDZp0iTrZfW5/fv3a8GCBcrOzpbP59POnTsj7nfOae3atcrKytKIESNUUFCgY8eO2Sy2D33RcVi2bNkFz4/58+fbLLaPlJaWasaMGUpKSlJ6eroWLlyompqaiH3a29tVXFyskSNH6pprrtHixYvV3NxstOK+8WWOw5w5cy54PjzyyCNGK+5dXATo9ddf15o1a7Ru3TodOnRI06ZNU2FhoU6dOmW9tH534403qrGxMby9++671kvqc21tbZo2bZo2btzY6/0bNmzQiy++qFdeeUUHDhzQ1VdfrcLCQrW3t/fzSvvWFx0HSZo/f37E82Pr1q39uMK+V1FRoeLiYlVVVWnv3r3q7OzUvHnz1NbWFt5n9erVeuutt7R9+3ZVVFTo5MmTuvvuuw1XHXtf5jhI0kMPPRTxfNiwYYPRii/CxYGZM2e64uLi8NddXV0uOzvblZaWGq6q/61bt85NmzbNehmmJLkdO3aEv+7u7naZmZnu+eefD9/W0tLi/H6/27p1q8EK+8fnj4Nzzi1dutTdddddJuuxcurUKSfJVVRUOOd6/t0PGzbMbd++PbzP73//eyfJVVZWWi2zz33+ODjn3Ne//nX36KOP2i3qSxjwZ0Dnz59XdXW1CgoKwrclJCSooKBAlZWVhiuzcezYMWVnZ2vcuHF64IEHdPz4ceslmaqvr1dTU1PE8yMQCCgvL++KfH6Ul5crPT1dEydO1PLly3XmzBnrJfWpYDAoSUpNTZUkVVdXq7OzM+L5MGnSJI0ZM2ZQPx8+fxw+8dprryktLU2TJ09WSUmJzp07Z7G8ixpwFyP9vNOnT6urq0sZGRkRt2dkZOgPf/iD0aps5OXlafPmzZo4caIaGxu1fv163XbbbTp69KiSkpKsl2eiqalJknp9fnxy35Vi/vz5uvvuu5Wbm6u6ujr98Ic/VFFRkSorKzVkyBDr5cVcd3e3Vq1apVmzZmny5MmSep4PiYmJSklJidh3MD8fejsOknT//fdr7Nixys7O1pEjR/TEE0+opqZGb775puFqIw34AOFTRUVF4T9PnTpVeXl5Gjt2rN544w09+OCDhivDQLBkyZLwn6dMmaKpU6dq/PjxKi8v19y5cw1X1jeKi4t19OjRK+J10Eu52HF4+OGHw3+eMmWKsrKyNHfuXNXV1Wn8+PH9vcxeDfgfwaWlpWnIkCEXvIulublZmZmZRqsaGFJSUnTDDTeotrbWeilmPnkO8Py40Lhx45SWljYonx8rVqzQ7t279c4770T8+pbMzEydP39eLS0tEfsP1ufDxY5Db/Ly8iRpQD0fBnyAEhMTNX36dJWVlYVv6+7uVllZmfLz8w1XZu/s2bOqq6tTVlaW9VLM5ObmKjMzM+L5EQqFdODAgSv++XHixAmdOXNmUD0/nHNasWKFduzYoX379ik3Nzfi/unTp2vYsGERz4eamhodP358UD0fvug49Obw4cOSNLCeD9bvgvgytm3b5vx+v9u8ebP73e9+5x5++GGXkpLimpqarJfWr773ve+58vJyV19f79577z1XUFDg0tLS3KlTp6yX1qdaW1vdhx9+6D788EMnyf3kJz9xH374ofvzn//snHPu2WefdSkpKW7Xrl3uyJEj7q677nK5ubnuo48+Ml55bF3qOLS2trrHHnvMVVZWuvr6evf222+7m2++2V1//fWuvb3deukxs3z5chcIBFx5eblrbGwMb+fOnQvv88gjj7gxY8a4ffv2uYMHD7r8/HyXn59vuOrY+6LjUFtb65555hl38OBBV19f73bt2uXGjRvnZs+ebbzySHERIOec+9nPfubGjBnjEhMT3cyZM11VVZX1kvrdvffe67KyslxiYqL7yle+4u69915XW1trvaw+98477zhJF2xLly51zvW8Ffupp55yGRkZzu/3u7lz57qamhrbRfeBSx2Hc+fOuXnz5rlRo0a5YcOGubFjx7qHHnpo0P1PWm///JLcpk2bwvt89NFH7rvf/a679tpr3VVXXeUWLVrkGhsb7RbdB77oOBw/ftzNnj3bpaamOr/f7yZMmOC+//3vu2AwaLvwz+HXMQAATAz414AAAIMTAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGDi/wNLEAYTpH6megAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image,label = dataset[0]\n",
    "plt.imshow(image,cmap='magma')\n",
    "print('label',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13ddc838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa05c3c0680>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image,label = dataset[49546]\n",
    "plt.imshow(image,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7836f34f",
   "metadata": {},
   "source": [
    "Summary of Data Evolution                                             \n",
    "Disk: Compressed Bytes (idx format).\n",
    "\n",
    "Dataset Load: PIL Image (28, 28), Integers 0-255.\n",
    "\n",
    "ToTensor: Tensor (1, 28, 28), Floats 0.0-1.0.\n",
    "\n",
    "Normalize: Tensor (1, 28, 28), Floats ~ -0.4 to 2.8.\n",
    "\n",
    "DataLoader: Tensor (64, 1, 28, 28), Ready for Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4031f05",
   "metadata": {},
   "source": [
    "## üíæ Understanding Image Tensor Structure (C, H, W)\n",
    "\n",
    "When working with images in PyTorch, the raw pixel data must be converted into a tensor structure that the **Convolutional Neural Network (CNN)** can understand and process efficiently. This is why the `transforms.ToTensor()` step adds the **Channel (C)** dimension and reorders the structure.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. The Channel Dimension ($\\mathbf{C}$)\n",
    "\n",
    "The channel dimension is added to specify the type of information stored per pixel.\n",
    "\n",
    "| Image Type | Channel Count ($\\mathbf{C}$) | Example Shape (Single Image) | Rationale |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Grayscale** (MNIST) | $\\mathbf{1}$ | $\\mathbf{(1, 28, 28)}$ | Stores only **luminance** (brightness). |\n",
    "| **Color** (RGB) | $\\mathbf{3}$ | $\\mathbf{(3, H, W)}$ | Stores **Red, Green, and Blue** intensities separately. |\n",
    "\n",
    "\n",
    "\n",
    "[Image of RGB color channels separated]\n",
    "\n",
    "\n",
    "### 2. The Final Tensor Form (4D Structure)\n",
    "\n",
    "The goal of the transformation pipeline is to provide the CNN with a spatially intact data structure.\n",
    "\n",
    "* **Classic Neural Networks (MLP):** These require a flattened **1D array** (e.g., $784$ features) because they treat every pixel independently.\n",
    "* **Convolutional Neural Networks (CNN):** These require the **spatial adjacency** of pixels to be preserved so the convolutional **filters (kernels)** can scan and detect local patterns (edges, corners).\n",
    "\n",
    "Therefore, the final data structure, when passed to the model in batches, is a **4D Tensor**:\n",
    "\n",
    "$$\\mathbf{(B, C, H, W)}$$\n",
    "\n",
    "Where:\n",
    "* $\\mathbf{B}$: **Batch Size** (The number of images processed simultaneously).\n",
    "* $\\mathbf{C}$: **Channels** (1 for MNIST).\n",
    "* $\\mathbf{H}$: **Height** (28 pixels).\n",
    "* $\\mathbf{W}$: **Width** (28 pixels).\n",
    "\n",
    "This 4D structure is the standard input requirement for almost all modern image classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d883db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba52850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root='/content/data',train=True,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb64d559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 4\n"
     ]
    }
   ],
   "source": [
    "img_tensor,label = dataset[342]\n",
    "print(img_tensor.shape,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "715c681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0784, 0.6745, 0.0471, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.4588, 0.6941, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.1647, 0.9451, 0.3647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.2784, 0.8627, 0.0471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0275, 0.2745, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.6314, 0.7529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.1412, 0.9412, 0.1176, 0.0000, 0.0000, 0.2588,\n",
      "          0.9882, 0.7529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.4157, 0.9569, 0.1569, 0.0000, 0.0000, 0.3137,\n",
      "          0.9961, 0.4824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.4863, 0.9961, 0.8118, 0.7569, 0.5176, 0.5961,\n",
      "          0.9961, 0.4118, 0.0000, 0.0000, 0.2745, 0.3961, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0078, 0.6196, 0.9961, 0.9804, 0.9608, 0.9961, 0.9961,\n",
      "          0.9961, 0.7490, 0.4510, 0.7216, 0.8941, 0.1294, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.4118, 0.9961, 0.8039, 0.1804, 0.0118, 0.4275, 0.9961,\n",
      "          0.9961, 0.9961, 0.9961, 0.9333, 0.3255, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.1373, 0.9529, 0.9569, 0.2627, 0.0000, 0.0000, 0.0000, 0.8824,\n",
      "          0.9137, 0.3686, 0.2745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.5373, 0.9961, 0.7608, 0.0000, 0.0000, 0.0000, 0.0000, 0.9961,\n",
      "          0.7176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.8706, 0.9961, 0.3137, 0.0000, 0.0000, 0.0000, 0.3255, 0.9961,\n",
      "          0.7176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.5882, 0.5490, 0.0196, 0.0000, 0.0000, 0.0000, 0.3451, 1.0000,\n",
      "          0.7176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4078, 0.9961,\n",
      "          0.5412, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6863, 0.9961,\n",
      "          0.3294, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.7922, 0.8902,\n",
      "          0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0510, 0.9216, 0.7608,\n",
      "          0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3804, 0.9961, 0.6863,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1294, 0.7686, 0.2157,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0fba699",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 28, 28) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3338558573.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   3590\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3591\u001b[0m ) -> AxesImage:\n\u001b[0;32m-> 3592\u001b[0;31m     __ret = gca().imshow(\n\u001b[0m\u001b[1;32m   3593\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3594\u001b[0m         \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m             return func(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                 \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5943\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_aspect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5945\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5946\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Needed e.g. to apply png palette.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_image_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# If just (M, N, 1), assume scalar and apply colormap.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid shape {A.shape} for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;31m# If the input data has values outside the valid range (after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 28, 28) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_tensor,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a8702e",
   "metadata": {},
   "source": [
    "This error is happening because Matplotlib expects image data in shape (H, W) for grayscale or (H, W, 3/4) for RGB/RGBA, but your tensor has shape (1, 28, 28) ‚Äî that extra leading 1 channel dimension is causing the problem.\n",
    "\n",
    "‚úÖ How to fix it\n",
    "You need to remove (or \"squeeze\") the channel dimension:\n",
    "plt.imshow(img_tensor.squeeze(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "- .squeeze() removes dimensions of size 1, so (1, 28, 28) becomes (28, 28).\n",
    "- Now Matplotlib sees it as a proper 2D grayscale image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53b0d6",
   "metadata": {},
   "source": [
    "grayscale is only 1 channel but usually other colours which we see are 3 channels they are r g b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b394c",
   "metadata": {},
   "source": [
    "train test split , we also need validation dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43aad1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_ds,val_ds = random_split(dataset,[50000,10000])\n",
    "len(train_ds),len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23e176db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size=128\n",
    "\n",
    "train_loader=DataLoader(train_ds,batch_size,shuffle=True)\n",
    "val_loader = DataLoader(val_ds,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277b45e",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now that we have prepared our data loaders, we can define our model.\n",
    "\n",
    "* A **logistic regression** model is almost identical to a linear regression model. It contains weights and bias matrices, and the output is obtained using simple matrix operations (`pred = x @ w.t() + b`). \n",
    "\n",
    "* As we did with linear regression, we can use `nn.Linear` to create the model instead of manually creating and initializing the matrices.\n",
    "\n",
    "* Since `nn.Linear` expects each training example to be a vector, each `1x28x28` image tensor is _flattened_ into a vector of size 784 `(28*28)` before being passed into the model. \n",
    "\n",
    "* The output for each image is a vector of size 10, with each element signifying the probability of a particular target label (i.e., 0 to 9). The predicted label for an image is simply the one with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15a01f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "#logistic regression model\n",
    "model = nn.Linear(input_size,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "141677cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Linear(in_features=784, out_features=10, bias=True)>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98a4c016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    }
   ],
   "source": [
    "print(model.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e48eb29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0217,  0.0219, -0.0285,  ..., -0.0293,  0.0204,  0.0092],\n",
       "        [ 0.0045, -0.0242,  0.0073,  ...,  0.0154, -0.0070,  0.0033],\n",
       "        [-0.0233, -0.0176, -0.0301,  ...,  0.0038,  0.0038,  0.0083],\n",
       "        ...,\n",
       "        [-0.0130, -0.0286,  0.0271,  ...,  0.0053,  0.0124,  0.0042],\n",
       "        [-0.0163, -0.0007, -0.0129,  ..., -0.0009, -0.0201, -0.0301],\n",
       "        [-0.0211,  0.0319,  0.0200,  ..., -0.0205,  0.0314,  0.0188]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6335984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0302,  0.0030,  0.0122, -0.0352, -0.0086,  0.0090,  0.0275, -0.0212,\n",
       "        -0.0136, -0.0200], requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8244780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 1, 2, 3, 9, 9, 2, 1, 9, 3, 9, 2, 6, 8, 6, 7, 2, 3, 0, 1, 6, 4, 2, 1,\n",
      "        9, 8, 6, 4, 7, 8, 2, 9, 7, 4, 4, 3, 1, 1, 8, 6, 2, 9, 2, 8, 1, 7, 6, 8,\n",
      "        3, 8, 5, 3, 7, 3, 7, 7, 6, 9, 9, 8, 2, 3, 0, 6, 7, 6, 4, 5, 6, 2, 6, 0,\n",
      "        4, 5, 6, 5, 5, 6, 0, 9, 3, 0, 9, 5, 0, 2, 4, 9, 4, 7, 9, 9, 2, 4, 8, 6,\n",
      "        5, 7, 1, 6, 1, 1, 7, 3, 9, 2, 2, 3, 9, 4, 3, 0, 1, 5, 5, 4, 3, 9, 4, 5,\n",
      "        3, 5, 4, 5, 0, 6, 0, 9])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2582145966.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)"
     ]
    }
   ],
   "source": [
    "for images,label in train_loader:\n",
    "    print(label)\n",
    "    print(images.shape)\n",
    "    print(model(images))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30769ec9",
   "metadata": {},
   "source": [
    "The code above leads to an error because our input data does not have the right shape. Our images are of the shape 1x28x28, but we need them to be vectors of size 784, i.e., we need to flatten them. We'll use the `.reshape` method of a tensor, which will allow us to efficiently 'view' each image as a flat vector without really creating a copy of the underlying data. To include this additional functionality within our model, we need to define a custom model by extending the `nn.Module` class from PyTorch. \n",
    "\n",
    "A class in Python provides a \"blueprint\" for creating objects. Let's look at an example of defining a new class in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1be9da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = images.reshape(128,784)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4cebd69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size,num_classes)\n",
    "\n",
    "    def forward(self,xb):\n",
    "        xb = xb.reshape(128,784)\n",
    "        out = self.linear(xb)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1307820",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0257e1",
   "metadata": {},
   "source": [
    "Inside the `__init__` constructor method, we instantiate the weights and biases using `nn.Linear`. And inside the `forward` method, which is invoked when we pass a batch of inputs to the model, we flatten the input tensor and pass it into `self.linear`.\n",
    "\n",
    "`xb.reshape(-1, 28*28)` indicates to PyTorch that we want a *view* of the `xb` tensor with two dimensions. The length along the 2nd dimension is 28\\*28 (i.e., 784). One argument to `.reshape` can be set to `-1` (in this case, the first dimension) to let PyTorch figure it out automatically based on the shape of the original tensor.\n",
    "\n",
    "Note that the model no longer has `.weight` and `.bias` attributes (as they are now inside the `.linear` attribute), but it does have a `.parameters` method that returns a list containing the weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "951fc08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0012,  0.0080, -0.0341,  ..., -0.0029,  0.0292,  0.0320],\n",
       "         [-0.0211, -0.0080, -0.0102,  ..., -0.0094,  0.0088,  0.0129],\n",
       "         [ 0.0276, -0.0333, -0.0292,  ..., -0.0257, -0.0253, -0.0179],\n",
       "         ...,\n",
       "         [-0.0333,  0.0257,  0.0062,  ..., -0.0298,  0.0330, -0.0040],\n",
       "         [ 0.0233,  0.0268,  0.0014,  ...,  0.0255, -0.0075,  0.0222],\n",
       "         [-0.0267,  0.0117, -0.0015,  ...,  0.0143, -0.0040, -0.0051]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0003, -0.0044,  0.0018,  0.0085, -0.0299, -0.0085, -0.0007, -0.0009,\n",
       "          0.0256,  0.0243], requires_grad=True)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbce3a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape :  torch.Size([128, 10])\n",
      "Sample outputs :\n",
      " tensor([[ 0.0943,  0.2378,  0.0422, -0.0703,  0.1660, -0.1327, -0.1530,  0.0365,\n",
      "          0.0362,  0.1699],\n",
      "        [ 0.4040,  0.1805,  0.0515, -0.1358,  0.5868,  0.1992,  0.0115,  0.1621,\n",
      "         -0.0575,  0.4989]])\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for images, labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    break\n",
    "\n",
    "print('outputs.shape : ', outputs.shape)\n",
    "print('Sample outputs :\\n', outputs[:2].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5223a63f",
   "metadata": {},
   "source": [
    "For each of the 100 input images, we get 10 outputs, one for each class. As discussed earlier, we'd like these outputs to represent probabilities. Each output row's elements must lie between 0 to 1 and add up to 1, which is not the case. \n",
    "\n",
    "To convert the output rows into probabilities, we use the softmax function, which has the following formula:\n",
    "\n",
    "![softmax](https://i.imgur.com/EAh9jLN.png)\n",
    "\n",
    "First, we replace each element `yi` in an output row by `e^yi`, making all the elements positive. \n",
    "\n",
    "![](https://www.montereyinstitute.org/courses/DevelopmentalMath/COURSE_TEXT2_RESOURCE/U18_L1_T1_text_final_6_files/image001.png)\n",
    "\n",
    "\n",
    "\n",
    "Then, we divide them by their sum to ensure that they add up to 1. The resulting vector can thus be interpreted as probabilities.\n",
    "\n",
    "While it's easy to implement the softmax function (you should try it!), we'll use the implementation that's provided within PyTorch because it works well with multidimensional tensors (a list of output rows in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec4039fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24e67eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(outputs,dim =1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29b2c3",
   "metadata": {},
   "source": [
    "dim one means apply softmax to columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "397f326d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 0, 9, 0, 0, 4, 0, 0, 0, 9, 4, 9, 4, 4, 9, 4, 1, 4, 4, 0, 4, 3, 4,\n",
      "        4, 9, 4, 9, 4, 4, 3, 0, 9, 5, 4, 4, 4, 0, 5, 4, 9, 4, 3, 4, 2, 9, 5, 9,\n",
      "        4, 4, 4, 0, 0, 4, 4, 1, 4, 8, 4, 8, 4, 5, 0, 5, 4, 9, 4, 5, 0, 0, 0, 0,\n",
      "        0, 0, 0, 4, 4, 4, 4, 4, 9, 1, 4, 9, 4, 4, 8, 4, 4, 4, 1, 1, 4, 4, 0, 0,\n",
      "        0, 4, 4, 0, 0, 4, 4, 4, 4, 0, 0, 4, 4, 0, 8, 4, 4, 9, 4, 0, 4, 9, 0, 4,\n",
      "        4, 4, 9, 6, 9, 9, 5, 0])\n",
      "tensor([0.1206, 0.1448, 0.1230, 0.1252, 0.1171, 0.1299, 0.1253, 0.1289, 0.1332,\n",
      "        0.1182, 0.1257, 0.1393, 0.1379, 0.1227, 0.1228, 0.1223, 0.1127, 0.1237,\n",
      "        0.1353, 0.1513, 0.1184, 0.1366, 0.1384, 0.1195, 0.1556, 0.1323, 0.1186,\n",
      "        0.1195, 0.1412, 0.1338, 0.1246, 0.1235, 0.1489, 0.1324, 0.1435, 0.1766,\n",
      "        0.1244, 0.1419, 0.1287, 0.1573, 0.1454, 0.1186, 0.1251, 0.1271, 0.1167,\n",
      "        0.1278, 0.1257, 0.1303, 0.1303, 0.1310, 0.1449, 0.1181, 0.1208, 0.1260,\n",
      "        0.1325, 0.1401, 0.1399, 0.1203, 0.1399, 0.1139, 0.1402, 0.1219, 0.1228,\n",
      "        0.1421, 0.1392, 0.1195, 0.1376, 0.1275, 0.1368, 0.1194, 0.1260, 0.1184,\n",
      "        0.1151, 0.1281, 0.1167, 0.1289, 0.1475, 0.1605, 0.1084, 0.1736, 0.1200,\n",
      "        0.1180, 0.1304, 0.1161, 0.1456, 0.1313, 0.1194, 0.1272, 0.1227, 0.1185,\n",
      "        0.1377, 0.1214, 0.1481, 0.1577, 0.1267, 0.1229, 0.1176, 0.1406, 0.1418,\n",
      "        0.1377, 0.1328, 0.1524, 0.1271, 0.1179, 0.1226, 0.1329, 0.1153, 0.1419,\n",
      "        0.1277, 0.1457, 0.1184, 0.1663, 0.1207, 0.1516, 0.1157, 0.1172, 0.1217,\n",
      "        0.1279, 0.1196, 0.1332, 0.1455, 0.1193, 0.1388, 0.1112, 0.1264, 0.1253,\n",
      "        0.1548, 0.1308], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
